{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"board-game-merger","text":"<p>Documentation: https://MarkusShepherd.github.io/board-game-merger</p> <p>Source Code: https://github.com/MarkusShepherd/board-game-merger</p> <p>PyPI: https://pypi.org/project/board-game-merger/</p> <p>Merging board game data</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install board-game-merger\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.9+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the docs directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github Pages page automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatting (<code>ruff format</code>), linters (e.g. <code>ruff</code> and <code>mypy</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This project was generated using the wolt-python-package-cookiecutter template.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#board_game_merger.merge","title":"<code>merge</code>","text":""},{"location":"api_docs/#board_game_merger.merge.merge_files","title":"<code>merge_files(*, merge_config: MergeConfig, overwrite: bool = False, drop_empty: bool = False, sort_keys: bool = False, progress_bar: bool = False) -&gt; None</code>","text":"<p>Merge files into one. Execute the following steps:</p> <ul> <li>Filter out rows older than latest_min</li> <li>For each row with identical keys, keep the latest one</li> <li>Sort the output by keys, latest, or fields</li> <li>Select only specified fields or exclude some fields</li> <li>For each row, remove empty fields and sort keys alphabetically</li> </ul> Source code in <code>src/board_game_merger/merge.py</code> <pre><code>def merge_files(\n    *,\n    merge_config: MergeConfig,\n    overwrite: bool = False,\n    drop_empty: bool = False,\n    sort_keys: bool = False,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"\n    Merge files into one. Execute the following steps:\n\n    - Filter out rows older than latest_min\n    - For each row with identical keys, keep the latest one\n    - Sort the output by keys, latest, or fields\n    - Select only specified fields or exclude some fields\n    - For each row, remove empty fields and sort keys alphabetically\n    \"\"\"\n\n    if (\n        merge_config.fieldnames_include is not None\n        and merge_config.fieldnames_exclude is not None\n    ):\n        msg = \"Cannot specify both fieldnames_include and fieldnames_exclude\"\n        raise ValueError(msg)\n\n    in_paths_iter = (\n        merge_config.in_paths\n        if isinstance(merge_config.in_paths, list)\n        else [merge_config.in_paths]\n    )\n    in_paths = [Path(in_path).resolve() for in_path in in_paths_iter]\n    out_path = Path(merge_config.out_path).resolve()\n\n    LOGGER.info(\n        \"Merging items from %s into &lt;%s&gt;\",\n        f\"[{len(in_paths)} paths]\" if len(in_paths) &gt; MAX_DISPLAY_ITEMS else in_paths,\n        out_path,\n    )\n\n    if not overwrite and out_path.exists():\n        LOGGER.warning(\"Output file already exists, use overwrite to replace it\")\n        return\n\n    data = pl.scan_ndjson(\n        source=in_paths,\n        schema=merge_config.schema,\n        batch_size=512,\n        low_memory=True,\n        rechunk=True,\n        ignore_errors=True,\n    )\n\n    latest_col = (\n        merge_config.latest_col\n        if isinstance(merge_config.latest_col, list)\n        else [merge_config.latest_col]\n    )\n    if merge_config.latest_min is not None:\n        LOGGER.info(\"Filtering out rows before &lt;%s&gt;\", merge_config.latest_min)\n        data = data.filter(latest_col[0] &gt;= merge_config.latest_min)\n\n    key_col = (\n        merge_config.key_col\n        if isinstance(merge_config.key_col, list)\n        else [merge_config.key_col]\n    )\n    key_col_dict = {f\"__key__{i}\": key for i, key in enumerate(key_col)}\n\n    LOGGER.info(\"Merging rows with identical keys: %s\", key_col)\n    LOGGER.info(\"Keeping latest by: %s\", latest_col)\n\n    data = (\n        data.sort(by=latest_col, descending=True, nulls_last=True)\n        .with_columns(**key_col_dict)\n        .unique(subset=list(key_col_dict), keep=\"first\")\n        .drop(key_col_dict.keys())\n    )\n\n    if merge_config.sort_fields is not None:\n        LOGGER.info(\n            \"Sorting data by: %s (%s)\",\n            merge_config.sort_fields,\n            \"descending\" if merge_config.sort_descending else \"ascending\",\n        )\n        data = data.sort(\n            merge_config.sort_fields,\n            descending=merge_config.sort_descending,\n        )\n\n    if merge_config.fieldnames_include is not None:\n        LOGGER.info(\"Selecting fields: %s\", merge_config.fieldnames_include)\n        data = data.select(merge_config.fieldnames_include)\n    elif merge_config.fieldnames_exclude is not None:\n        LOGGER.info(\"Excluding fields: %s\", merge_config.fieldnames_exclude)\n        data = data.select(pl.exclude(merge_config.fieldnames_exclude))\n\n    LOGGER.info(\"Collecting results, this may take a while\u2026\")\n    result = data.collect()\n    LOGGER.info(\"Finished collecting results with shape %dx%d\", *result.shape)\n    num_rows = len(result)\n\n    if not drop_empty and not sort_keys:\n        LOGGER.info(\"Writing merged data to &lt;%s&gt;\", out_path)\n        result.write_ndjson(out_path)\n        LOGGER.info(\"Done.\")\n        return\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_file = Path(temp_dir) / \"merged.jl\"\n        LOGGER.info(\"Writing merged data to &lt;%s&gt;\", temp_file)\n        result.write_ndjson(temp_file)\n        del result\n\n        LOGGER.info(\"Writing cleaned data to &lt;%s&gt;\", out_path)\n        with temp_file.open(\"r\") as in_file, out_path.open(\"w\") as out_file:\n            lines = (\n                tqdm(\n                    in_file,\n                    desc=\"Cleaning data\",\n                    unit=\" rows\",\n                    total=num_rows,\n                )\n                if progress_bar\n                else in_file\n            )\n            for line in lines:\n                row = json.loads(line)\n                if drop_empty:\n                    row = {k: v for k, v in row.items() if v}\n                json.dump(row, out_file, sort_keys=sort_keys, separators=(\",\", \":\"))\n                out_file.write(\"\\n\")\n\n    LOGGER.info(\"Done.\")\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#100-2024-11-10","title":"1.0.0 - 2024-11-10","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release</li> </ul>"}]}